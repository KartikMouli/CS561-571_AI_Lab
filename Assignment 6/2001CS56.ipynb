{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Rohit\n",
      "[nltk_data]     Ranjan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Rohit\n",
      "[nltk_data]     Ranjan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Rohit Ranjan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import classification_report, f1_score, precision_score, recall_score, accuracy_score, confusion_matrix\n",
    "\n",
    "# Import NLTK libraries for natural language processing\n",
    "from nltk import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "import string\n",
    "import operator\n",
    "from copy import deepcopy\n",
    "from math import log2\n",
    "from statistics import mean\n",
    "from collections import Counter\n",
    "\n",
    "# Download NLTK resources\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Training data...\n",
      "Training Data:\n",
      "[['DESC', 'How did serfdom develop in and then leave Russia', 9, [('How',), ('did',), ('serfdom',), ('develop',), ('in',), ('and',), ('then',), ('leave',), ('Russia',)], [('How', 'did'), ('did', 'serfdom'), ('serfdom', 'develop'), ('develop', 'in'), ('in', 'and'), ('and', 'then'), ('then', 'leave'), ('leave', 'Russia')], [('How', 'did', 'serfdom'), ('did', 'serfdom', 'develop'), ('serfdom', 'develop', 'in'), ('develop', 'in', 'and'), ('in', 'and', 'then'), ('and', 'then', 'leave'), ('then', 'leave', 'Russia')], [('How', 'WRB'), ('serfdom', 'JJ'), ('develop', 'VB'), ('leave', 'JJ'), ('Russia', 'NNP')]], ['ENTY', 'What films featured the character Popeye Doyle', 7, [('What',), ('films',), ('featured',), ('the',), ('character',), ('Popeye',), ('Doyle',)], [('What', 'films'), ('films', 'featured'), ('featured', 'the'), ('the', 'character'), ('character', 'Popeye'), ('Popeye', 'Doyle')], [('What', 'films', 'featured'), ('films', 'featured', 'the'), ('featured', 'the', 'character'), ('the', 'character', 'Popeye'), ('character', 'Popeye', 'Doyle')], [('What', 'WP'), ('films', 'VBD'), ('featured', 'JJ'), ('character', 'NN'), ('Popeye', 'NNP'), ('Doyle', 'NNP')]], ['DESC', 'How can I find a list of celebrities  real names', 11, [('How',), ('can',), ('I',), ('find',), ('a',), ('list',), ('of',), ('celebrities',), ('real',), ('names',)], [('How', 'can'), ('can', 'I'), ('I', 'find'), ('find', 'a'), ('a', 'list'), ('list', 'of'), ('of', 'celebrities'), ('celebrities', 'real'), ('real', 'names')], [('How', 'can', 'I'), ('can', 'I', 'find'), ('I', 'find', 'a'), ('find', 'a', 'list'), ('a', 'list', 'of'), ('list', 'of', 'celebrities'), ('of', 'celebrities', 'real'), ('celebrities', 'real', 'names')], [('How', 'WRB'), ('I', 'PRP'), ('find', 'VBP'), ('list', 'JJ'), ('celebrities', 'NNS'), ('real', 'JJ'), ('names', 'NNS')]], ['ENTY', 'What fowl grabs the spotlight after the Chinese Year of the Monkey', 12, [('What',), ('fowl',), ('grabs',), ('the',), ('spotlight',), ('after',), ('the',), ('Chinese',), ('Year',), ('of',), ('the',), ('Monkey',)], [('What', 'fowl'), ('fowl', 'grabs'), ('grabs', 'the'), ('the', 'spotlight'), ('spotlight', 'after'), ('after', 'the'), ('the', 'Chinese'), ('Chinese', 'Year'), ('Year', 'of'), ('of', 'the'), ('the', 'Monkey')], [('What', 'fowl', 'grabs'), ('fowl', 'grabs', 'the'), ('grabs', 'the', 'spotlight'), ('the', 'spotlight', 'after'), ('spotlight', 'after', 'the'), ('after', 'the', 'Chinese'), ('the', 'Chinese', 'Year'), ('Chinese', 'Year', 'of'), ('Year', 'of', 'the'), ('of', 'the', 'Monkey')], [('What', 'WP'), ('fowl', 'NN'), ('grabs', 'NN'), ('spotlight', 'NN'), ('Chinese', 'JJ'), ('Year', 'NNP'), ('Monkey', 'NNP')]], ['ABBR', 'What is the full form of com', 7, [('What',), ('is',), ('the',), ('full',), ('form',), ('of',), ('com',)], [('What', 'is'), ('is', 'the'), ('the', 'full'), ('full', 'form'), ('form', 'of'), ('of', 'com')], [('What', 'is', 'the'), ('is', 'the', 'full'), ('the', 'full', 'form'), ('full', 'form', 'of'), ('form', 'of', 'com')], [('What', 'WP'), ('full', 'JJ'), ('form', 'NN'), ('com', 'NN')]]]\n"
     ]
    }
   ],
   "source": [
    "# Define a function to extract n-grams from text data\n",
    "def get_ngrams(data, n):\n",
    "    tokens = [token for token in data.split(\" \") if token != \"\"]\n",
    "    return list(ngrams(tokens, n))\n",
    "\n",
    "# Define a function to extract part-of-speech tags from text data\n",
    "def get_postag(txt):\n",
    "    # Define a set of English stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # Tokenize the input text into sentences\n",
    "    tokenized = sent_tokenize(txt)\n",
    "\n",
    "    # Tokenize the first sentence into words and filter out stopwords\n",
    "    words_list = nltk.word_tokenize(tokenized[0]) \n",
    "    words_list = [w for w in words_list if not w in stop_words]  \n",
    "\n",
    "    # Get part-of-speech tags for the filtered words\n",
    "    return nltk.pos_tag(words_list)\n",
    "\n",
    "# Define a function to build data from a file\n",
    "def build_data(path_to_data):\n",
    "    data,uni,bi,tri,pos = [],[],[],[],[]\n",
    "    file = open(path_to_data)\n",
    "\n",
    "    for line in file:\n",
    "        line = line.split(':')\n",
    "        row = []\n",
    "        \n",
    "        # Extract the class and question from the input line\n",
    "        _class, _question = line[0], line[1]\n",
    "        row.append(_class)\n",
    "        row.append(' '.join(_question.split(' ')[1:]).translate(str.maketrans('', '', string.punctuation)).rstrip())\n",
    "\n",
    "        # Calculate the length of the question\n",
    "        length = len(row[1].split(' '))\n",
    "        row.append(length)\n",
    "\n",
    "        # Extract unigrams, bigrams, and trigrams from the question\n",
    "        unigram = get_ngrams(row[1], 1)\n",
    "        row.append(unigram)\n",
    "        uni.extend(unigram)\n",
    "\n",
    "        bigram = get_ngrams(row[1], 2)\n",
    "        row.append(bigram)\n",
    "        bi.extend(bigram)\n",
    "        \n",
    "        trigram = get_ngrams(row[1], 3)\n",
    "        row.append(trigram)\n",
    "        tri.extend(trigram)\n",
    "\n",
    "        # Extract part-of-speech tags from the question\n",
    "        postag = get_postag(row[1])\n",
    "        row.append(postag)\n",
    "        pos.extend(postag)\n",
    " \n",
    "        data.append(row)\n",
    "\n",
    "    return data, uni, bi, tri, pos\n",
    "\n",
    "# Call the build_data function to load and process training data\n",
    "data, uni, bi, tri, pos = build_data('./train_data.txt')\n",
    "\n",
    "# Print some information about the loaded training data\n",
    "print('Loading Training data...')\n",
    "print('Training Data:')\n",
    "print(data[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average length: 9.031548055759353\n",
      "Top features:\n",
      "\n",
      "Unigrams:\n",
      "\n",
      "[(('the',), 3589), (('What',), 3245), (('is',), 1669), (('of',), 1540), (('in',), 1131)]\n",
      "Bigrams:\n",
      "\n",
      "[(('What', 'is'), 968), (('is', 'the'), 757), (('of', 'the'), 446), (('in', 'the'), 326), (('How', 'many'), 316)]\n",
      "Trigrams:\n",
      "\n",
      "[(('What', 'is', 'the'), 551), (('What', 'is', 'a'), 151), (('What', 's', 'the'), 135), (('What', 'are', 'the'), 134), (('What', 'was', 'the'), 130)]\n",
      "Pos Counts:\n",
      "\n",
      "[(('What', 'WP'), 3245), (('How', 'WRB'), 763), (('Who', 'WP'), 559), (('many', 'JJ'), 332), (('Where', 'WRB'), 273)]\n"
     ]
    }
   ],
   "source": [
    "# Define a function to find the top n-grams from a list of n-grams\n",
    "def top_grams(grams, top_n):\n",
    "    return Counter(grams).most_common(top_n)\n",
    "\n",
    "# Calculate the top 500 unigrams, 300 bigrams, 200 trigrams, and 500 POS tags\n",
    "unigram_counts = top_grams(uni, 500)\n",
    "bigram_counts = top_grams(bi, 300)\n",
    "trigram_counts = top_grams(tri, 200)\n",
    "pos_counts = top_grams(pos, 500)\n",
    "\n",
    "# Calculate the average length of questions in the dataset\n",
    "avg_length = mean([row[2] for row in data])\n",
    "print('average length:',avg_length)\n",
    "\n",
    "# Display the top features\n",
    "print('Top features:\\n')\n",
    "print('Unigrams:\\n')\n",
    "print(unigram_counts[0:5])\n",
    "\n",
    "print('Bigrams:\\n')\n",
    "print(bigram_counts[0:5])\n",
    "\n",
    "print('Trigrams:\\n')\n",
    "print(trigram_counts[0:5])\n",
    "\n",
    "print('Pos Counts:\\n')\n",
    "print(pos_counts[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to check if a value is numeric (int or float)\n",
    "def is_numeric(value):\n",
    "    return isinstance(value, int) or isinstance(value, float)\n",
    "\n",
    "# Define a list of column headers\n",
    "header = ['Label', 'Text', 'Length', 'Unigram', 'Bigram', 'Trigram', 'POS']\n",
    "\n",
    "# Define a class to represent a question\n",
    "class Question:\n",
    "    def __init__(self, col, value):\n",
    "        # The column number in the header\n",
    "        self.col = col \n",
    "\n",
    "        # Actual value of the object\n",
    "        self.value = value \n",
    "\n",
    "    # Method to check if the attribute of the current question matches a given example\n",
    "    def match(self, example):\n",
    "        val = example[self.col]\n",
    "        if is_numeric(val):\n",
    "            return val <= self.value\n",
    "        \n",
    "        return self.value in val\n",
    "\n",
    "    # Method to return the string representation of the object\n",
    "    def __repr__(self):\n",
    "        condition = \"contains\"\n",
    "        return \"Does %s %s %s?\" % (\n",
    "            header[self.col], condition, str(self.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate class counts in a set of rows\n",
    "def class_counts(rows):\n",
    "    counts = {}\n",
    "    for row in rows:\n",
    "        # Assuming the label is in the first column\n",
    "        label = row[0]\n",
    "\n",
    "        if label not in counts:\n",
    "            counts[label] = 0\n",
    "        \n",
    "        counts[label] += 1\n",
    "    return counts\n",
    "\n",
    "# Function to calculate Gini impurity of a set of rows\n",
    "def gini(rows):\n",
    "    counts = class_counts(rows)\n",
    "    impurity = 1\n",
    "    for lbl in counts:\n",
    "        prob_of_lbl = counts[lbl] / float(len(rows))\n",
    "        impurity -= prob_of_lbl**2\n",
    "    return impurity\n",
    "\n",
    "# Function to calculate misclassification error of a set of rows\n",
    "def misclassifcation_error(rows):\n",
    "    counts = class_counts(rows)\n",
    "    max_prob = 0\n",
    "    for lbl in counts:\n",
    "        prob_of_lbl = counts[lbl] / float(len(rows))\n",
    "        if prob_of_lbl > max_prob:\n",
    "            max_prob = prob_of_lbl\n",
    "    return 1 - max_prob\n",
    "\n",
    "# Function to calculate entropy of a set of rows\n",
    "def entropy(rows):\n",
    "    counts = class_counts(rows)\n",
    "    impurity = 0\n",
    "    for lbl in counts:\n",
    "        prob_of_lbl = counts[lbl] / float(len(rows))\n",
    "        impurity -= prob_of_lbl*log2(prob_of_lbl)\n",
    "    return impurity\n",
    "\n",
    "# Function to calculate information gain based on a given impurity function\n",
    "def info_gain(left, right, current_uncertainty, func):\n",
    "    p = float(len(left))/(len(left)+len(right))\n",
    "    return current_uncertainty - p*func(left) - (1-p)*func(right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a class to represent a leaf node in a decision tree\n",
    "class Leaf:\n",
    "    def __init__(self, rows):\n",
    "        # Store predictions for this leaf node, which are the class counts for the rows it represents\n",
    "        self.predictions = class_counts(rows)\n",
    "\n",
    "# Define a class to represent a decision node in a decision tree\n",
    "class Decision_Node:\n",
    "    def __init__(self, question, true_branch, false_branch):\n",
    "        # Store the question used to split the data at this node\n",
    "        self.question = question\n",
    "\n",
    "        # Store the true branch, which is the subtree for examples that satisfy the question\n",
    "        self.true_branch = true_branch\n",
    "\n",
    "        # Store the false branch, which is the subtree for examples that do not satisfy the question\n",
    "        self.false_branch = false_branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1501\n",
      "[Does Unigram contains ('the',)?, Does Unigram contains ('What',)?, Does Unigram contains ('is',)?, Does Unigram contains ('of',)?, Does Unigram contains ('in',)?]\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to store questions\n",
    "questions = []\n",
    "\n",
    "# Create questions based on the top unigrams\n",
    "for x in unigram_counts:\n",
    "    questions.append(Question(3, x[0]))\n",
    "\n",
    "# Create questions based on the top bigrams\n",
    "for x in bigram_counts:\n",
    "    questions.append(Question(4, x[0]))\n",
    "\n",
    "# Create questions based on the top trigrams    \n",
    "for x in trigram_counts:\n",
    "    questions.append(Question(5, x[0]))\n",
    "\n",
    "# Create questions based on the top POS tags\n",
    "for x in pos_counts:\n",
    "    questions.append(Question(6, x[0]))\n",
    "\n",
    "# Create a question based on the average length of questions    \n",
    "questions.append(Question(2, avg_length))    \n",
    "\n",
    "# Print the total number of questions and the first five questions    \n",
    "print(len(questions))\n",
    "print(questions[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to partition a set of rows into two subsets based on a given question\n",
    "def partition(rows, question):\n",
    "    rows_true = []\n",
    "    rows_false = []\n",
    "    \n",
    "    for r in rows:\n",
    "        if question.match(r):\n",
    "            rows_true.append(r)\n",
    "        else:\n",
    "            rows_false.append(r)\n",
    "    \n",
    "    return rows_true, rows_false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the best split (question) among a list of questions for a set of rows\n",
    "def find_best_split(rows, questions, func):   \n",
    "    best_gain = 0\n",
    "    best_question = None\n",
    "    current_uncertainty = func(rows)\n",
    "    \n",
    "    for q in questions:\n",
    "        # Split the rows based on the current question\n",
    "        rows_true, rows_false = partition(rows, q)\n",
    "        \n",
    "        if len(rows_true) == 0 or len(rows_false) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Calculate the information gain for the current split\n",
    "        gain = info_gain(rows_true, rows_false, current_uncertainty, func) \n",
    "        \n",
    "        # Update the best gain and best question if the current gain is higher\n",
    "        if gain >= best_gain:\n",
    "            best_gain, best_question = gain, q\n",
    "    \n",
    "    return best_gain, best_question   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursive function to form a decision tree using partitioning and a list of questions\n",
    "def form_tree(rows, questions, func):\n",
    "    # Find the best gain and best question to split the current set of rows\n",
    "    gain, question = find_best_split(rows, questions, func)\n",
    "\n",
    "    # If there's no more information gain (gain is 0), create a leaf node with predictions\n",
    "    if gain == 0:\n",
    "        return Leaf(rows)\n",
    "    \n",
    "    # Split the current set of rows based on the best question\n",
    "    rows_true, rows_false = partition(rows, question)\n",
    "\n",
    "    # Remove the used question from the list of available questions\n",
    "    questions.remove(question)\n",
    "    \n",
    "    # Recursively build the true and false branches of the decision tree\n",
    "    true_branch = form_tree(rows_true, questions, func)\n",
    "    false_branch = form_tree(rows_false, questions, func)\n",
    "    \n",
    "    # Create a decision node with the best question and its branches\n",
    "    return Decision_Node(question, true_branch, false_branch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to classify a single row using a decision tree node\n",
    "def classify_row(node, row):\n",
    "    if isinstance(node, Leaf):\n",
    "        # Return the predictions stored in the leaf node\n",
    "        return node.predictions\n",
    "    \n",
    "    # Check if the row satisfies the question at the current node\n",
    "    if node.question.match(row):\n",
    "        # Recursively classify the row in the true branch\n",
    "        return classify_row(node.true_branch, row)\n",
    "    else:\n",
    "        # Recursively classify the row in the false branch\n",
    "        return classify_row(node.false_branch, row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train a decision tree using the provided data, list of questions, and impurity function\n",
    "def train(data, questions, func):\n",
    "    # Create and return the root node of the decision tree by calling the form_tree function\n",
    "    return form_tree(data, deepcopy(questions), func)\n",
    "\n",
    "# Function to classify a set of rows using a trained decision tree\n",
    "def classify(root, rows):\n",
    "    # Classify each row using the decision tree and return a list of predictions\n",
    "    predictions = [max(classify_row(root, r).items(), key=operator.itemgetter(1))[0] for r in rows]\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to retrieve data entries at specified indices from a data list\n",
    "def get_data_in_index(data, index):\n",
    "    l = []\n",
    "    for i in range(len(data)):\n",
    "        if i in index:\n",
    "            l.append(data[i])\n",
    "    return l\n",
    "\n",
    "# Function to extract actual labels from a list of data entries\n",
    "def get_actual_labels(act_data):\n",
    "    act_labels = []\n",
    "    \n",
    "    for d in act_data:\n",
    "        # Assuming the label is in the first column\n",
    "        act_labels.append(d[0])\n",
    "    \n",
    "    return act_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "Training...\n",
      "\n",
      "Gini Index\n",
      "Precision Score = 0.8040501395611818\n",
      "Recall Score = 0.7487842295806157\n",
      "F Score = 0.767009336109572\n"
     ]
    }
   ],
   "source": [
    "# Create a 10-fold cross-validation splitter\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize lists to store precision, recall, and F1 score for each fold\n",
    "precision,recall,f_score = [],[],[]\n",
    "\n",
    "# Loop through the folds\n",
    "for trainInd, testInd in kfold.split(data):\n",
    "    # Get training data for the current fold\n",
    "    train_data = get_data_in_index(data, trainInd)\n",
    "\n",
    "    # Get test data for the current fold\n",
    "    test_data = get_data_in_index(data, testInd)\n",
    "    \n",
    "    # Train a decision tree on the training data using the Gini index as the impurity function\n",
    "    root = train(train_data, questions, gini)\n",
    "\n",
    "    # Classify the test data using the trained decision tree\n",
    "    prediction = classify(root, test_data)\n",
    "\n",
    "    # Extract actual labels from the test data\n",
    "    actual = get_actual_labels(test_data)\n",
    "\n",
    "    predicted = prediction\n",
    "    \n",
    "    # Calculate precision, recall, and F1 score for the current fold and store them in the respective lists\n",
    "    precision.append(precision_score(actual, predicted, average='macro'))\n",
    "    recall.append(recall_score(actual, predicted, average='macro'))\n",
    "    f_score.append(f1_score(actual, predicted, average='macro'))\n",
    "     \n",
    "    print(\"Training...\")\n",
    "\n",
    "# Print the average precision, recall, and F1 score across all folds\n",
    "print('\\nGini Index')\n",
    "print(\"Precision Score = \"+str(mean(precision)))\n",
    "print(\"Recall Score = \"+str(mean(recall)))\n",
    "print(\"F Score = \"+str(mean(f_score)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    }
   ],
   "source": [
    "# Define a list of classes\n",
    "classes = ['ABBR', 'DESC', 'ENTY', 'HUM', 'LOC', 'NUM']\n",
    "\n",
    "# Function to generate a classification report and other statistics based on the provided flags\n",
    "def getReport(train_data, test_data, uniFlag=True, biFlag=True, triFlag=True, posFlag=True, lenFlag=True, func=gini):\n",
    "    allQuestions = []\n",
    "    \n",
    "    # Generate questions based on the specified flags\n",
    "    if uniFlag:\n",
    "        for x in unigram_counts:\n",
    "            allQuestions.append(Question(3, x[0]))\n",
    "\n",
    "    if biFlag:\n",
    "        for x in bigram_counts:\n",
    "            allQuestions.append(Question(4, x[0]))\n",
    "\n",
    "    if triFlag:\n",
    "        for x in trigram_counts:\n",
    "            allQuestions.append(Question(5, x[0]))\n",
    "\n",
    "    if posFlag:\n",
    "        for x in pos_counts:\n",
    "            allQuestions.append(Question(6, x[0]))\n",
    "\n",
    "    if lenFlag:\n",
    "        allQuestions.append(Question(2, avg_length))    \n",
    "\n",
    "    print(\"No of questions = \" + str(len(allQuestions)))\n",
    "\n",
    "    print(\"Training...\")\n",
    "\n",
    "    # Train a decision tree using specified data and questions\n",
    "    root = train(train_data, allQuestions, func)\n",
    "    \n",
    "    print(\"Predicting...\")\n",
    "\n",
    "    # Classify the test data using the trained decision tree\n",
    "    prediction = classify(root, test_data)        \n",
    "\n",
    "    actual = get_actual_labels(test_data)\n",
    "    \n",
    "    print(\"Prediction done...\")\n",
    "\n",
    "    # Create a confusion matrix\n",
    "    matrix = confusion_matrix(actual, prediction)\n",
    "\n",
    "    # Generate a classification report\n",
    "    class_report = classification_report(actual, prediction)\n",
    "\n",
    "    acc = matrix.diagonal()/matrix.sum(axis=1)\n",
    "\n",
    "    # Calculate and store accuracy for each class\n",
    "    accuracy_report = dict(zip(classes, acc))\n",
    "    \n",
    "    return accuracy_report, class_report, root, prediction, actual\n",
    "\n",
    "# Prepare test data\n",
    "test_data = build_data('./test_data.txt')[0]\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of questions = 1501\n",
      "Training...\n",
      "Predicting...\n",
      "Prediction done...\n",
      "{'ABBR': 0.6666666666666666, 'DESC': 0.9710144927536232, 'ENTY': 0.723404255319149, 'HUM': 0.8461538461538461, 'LOC': 0.7037037037037037, 'NUM': 0.8141592920353983}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        ABBR       0.86      0.67      0.75         9\n",
      "        DESC       0.76      0.97      0.85       138\n",
      "        ENTY       0.68      0.72      0.70        94\n",
      "         HUM       0.92      0.85      0.88        65\n",
      "         LOC       0.89      0.70      0.79        81\n",
      "         NUM       0.99      0.81      0.89       113\n",
      "\n",
      "    accuracy                           0.82       500\n",
      "   macro avg       0.85      0.79      0.81       500\n",
      "weighted avg       0.84      0.82      0.82       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Call getReport to obtain the results\n",
    "accuracy_report, class_report, root, prediction, actual = getReport(train_data=data, test_data=test_data)\n",
    "\n",
    "# Print the accuracy report and classification report\n",
    "print(accuracy_report)\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of questions = 1501\n",
      "Training...\n",
      "Predicting...\n",
      "Prediction done...\n",
      "{'ABBR': 0.6666666666666666, 'DESC': 0.9710144927536232, 'ENTY': 0.5, 'HUM': 0.8615384615384616, 'LOC': 0.7283950617283951, 'NUM': 0.8053097345132744}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        ABBR       0.86      0.67      0.75         9\n",
      "        DESC       0.66      0.97      0.79       138\n",
      "        ENTY       0.69      0.50      0.58        94\n",
      "         HUM       0.90      0.86      0.88        65\n",
      "         LOC       0.88      0.73      0.80        81\n",
      "         NUM       0.98      0.81      0.88       113\n",
      "\n",
      "    accuracy                           0.79       500\n",
      "   macro avg       0.83      0.76      0.78       500\n",
      "weighted avg       0.81      0.79      0.78       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Call getReport with entropy as the impurity measure\n",
    "accuracy_report, class_report, root, prediction, actual = getReport(train_data=data, test_data=test_data, func=entropy)\n",
    "\n",
    "# Print the accuracy report and classification report\n",
    "print(accuracy_report)\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of questions = 1501\n",
      "Training...\n",
      "Predicting...\n",
      "Prediction done...\n",
      "{'ABBR': 0.6666666666666666, 'DESC': 0.8260869565217391, 'ENTY': 0.7978723404255319, 'HUM': 0.8461538461538461, 'LOC': 0.691358024691358, 'NUM': 0.7876106194690266}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        ABBR       0.86      0.67      0.75         9\n",
      "        DESC       0.77      0.83      0.79       138\n",
      "        ENTY       0.57      0.80      0.66        94\n",
      "         HUM       0.92      0.85      0.88        65\n",
      "         LOC       0.92      0.69      0.79        81\n",
      "         NUM       0.98      0.79      0.87       113\n",
      "\n",
      "    accuracy                           0.79       500\n",
      "   macro avg       0.83      0.77      0.79       500\n",
      "weighted avg       0.82      0.79      0.80       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Call getReport with misclassifcation_error as the impurity measure\n",
    "accuracy_report, class_report, root, prediction, actual = getReport(train_data=data, test_data=test_data, func=misclassifcation_error)\n",
    "\n",
    "# Print the accuracy report and classification report\n",
    "print(accuracy_report)\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of questions = 1500\n",
      "Training...\n",
      "Predicting...\n",
      "Prediction done...\n",
      "{'ABBR': 0.6666666666666666, 'DESC': 0.9710144927536232, 'ENTY': 0.723404255319149, 'HUM': 0.8461538461538461, 'LOC': 0.7037037037037037, 'NUM': 0.8141592920353983}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        ABBR       0.86      0.67      0.75         9\n",
      "        DESC       0.76      0.97      0.85       138\n",
      "        ENTY       0.68      0.72      0.70        94\n",
      "         HUM       0.92      0.85      0.88        65\n",
      "         LOC       0.89      0.70      0.79        81\n",
      "         NUM       0.99      0.81      0.89       113\n",
      "\n",
      "    accuracy                           0.82       500\n",
      "   macro avg       0.85      0.79      0.81       500\n",
      "weighted avg       0.84      0.82      0.82       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Call getReport without using the length feature\n",
    "accuracy_report, class_report, root, prediction, actual = getReport(train_data=data, test_data=test_data, lenFlag=False)\n",
    "\n",
    "# Print the accuracy report and classification report\n",
    "print(accuracy_report)\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of questions = 1500\n",
      "Training...\n",
      "Predicting...\n",
      "Prediction done...\n",
      "{'ABBR': 0.6666666666666666, 'DESC': 0.9710144927536232, 'ENTY': 0.5, 'HUM': 0.8615384615384616, 'LOC': 0.7283950617283951, 'NUM': 0.8053097345132744}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        ABBR       0.86      0.67      0.75         9\n",
      "        DESC       0.66      0.97      0.79       138\n",
      "        ENTY       0.69      0.50      0.58        94\n",
      "         HUM       0.90      0.86      0.88        65\n",
      "         LOC       0.88      0.73      0.80        81\n",
      "         NUM       0.98      0.81      0.88       113\n",
      "\n",
      "    accuracy                           0.79       500\n",
      "   macro avg       0.83      0.76      0.78       500\n",
      "weighted avg       0.81      0.79      0.78       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Call getReport without using the length feature and with entropy as the impurity measure\n",
    "accuracy_report, class_report, root, prediction, actual = getReport(train_data=data, test_data=test_data, lenFlag=False, func=entropy)\n",
    "\n",
    "# Print the accuracy report and classification report\n",
    "print(accuracy_report)\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of questions = 1500\n",
      "Training...\n",
      "Predicting...\n",
      "Prediction done...\n",
      "{'ABBR': 0.6666666666666666, 'DESC': 0.8260869565217391, 'ENTY': 0.7978723404255319, 'HUM': 0.8461538461538461, 'LOC': 0.691358024691358, 'NUM': 0.7787610619469026}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        ABBR       0.86      0.67      0.75         9\n",
      "        DESC       0.77      0.83      0.79       138\n",
      "        ENTY       0.56      0.80      0.66        94\n",
      "         HUM       0.92      0.85      0.88        65\n",
      "         LOC       0.92      0.69      0.79        81\n",
      "         NUM       0.98      0.78      0.87       113\n",
      "\n",
      "    accuracy                           0.79       500\n",
      "   macro avg       0.83      0.77      0.79       500\n",
      "weighted avg       0.82      0.79      0.80       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Call getReport without using the length feature and with misclassification error as the impurity measure\n",
    "accuracy_report, class_report, root, prediction, actual = getReport(train_data=data, test_data=test_data, lenFlag=False, func=misclassifcation_error)\n",
    "\n",
    "# Print the accuracy report and classification report\n",
    "print(accuracy_report)\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of questions = 1000\n",
      "Training...\n",
      "Predicting...\n",
      "Prediction done...\n",
      "{'ABBR': 0.6666666666666666, 'DESC': 0.9782608695652174, 'ENTY': 0.6276595744680851, 'HUM': 0.8461538461538461, 'LOC': 0.654320987654321, 'NUM': 0.7699115044247787}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        ABBR       0.86      0.67      0.75         9\n",
      "        DESC       0.73      0.98      0.84       138\n",
      "        ENTY       0.60      0.63      0.61        94\n",
      "         HUM       0.87      0.85      0.86        65\n",
      "         LOC       0.88      0.65      0.75        81\n",
      "         NUM       1.00      0.77      0.87       113\n",
      "\n",
      "    accuracy                           0.79       500\n",
      "   macro avg       0.82      0.76      0.78       500\n",
      "weighted avg       0.81      0.79      0.79       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Call getReport without using the length feature and POS feature\n",
    "accuracy_report, class_report, root, prediction, actual = getReport(train_data=data, test_data=test_data, lenFlag=False, posFlag=False)\n",
    "\n",
    "# Print the accuracy report and classification report\n",
    "print(accuracy_report)\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of questions = 1000\n",
      "Training...\n",
      "Predicting...\n",
      "Prediction done...\n",
      "{'ABBR': 0.6666666666666666, 'DESC': 0.427536231884058, 'ENTY': 0.648936170212766, 'HUM': 0.8769230769230769, 'LOC': 0.6296296296296297, 'NUM': 0.7699115044247787}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        ABBR       0.86      0.67      0.75         9\n",
      "        DESC       0.57      0.43      0.49       138\n",
      "        ENTY       0.35      0.65      0.45        94\n",
      "         HUM       0.93      0.88      0.90        65\n",
      "         LOC       0.82      0.63      0.71        81\n",
      "         NUM       0.97      0.77      0.86       113\n",
      "\n",
      "    accuracy                           0.64       500\n",
      "   macro avg       0.75      0.67      0.69       500\n",
      "weighted avg       0.71      0.64      0.66       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Call getReport without using the length feature and POS feature, and with entropy as the impurity measure\n",
    "accuracy_report, class_report, root, prediction, actual = getReport(train_data=data, test_data=test_data, lenFlag=False, posFlag=False, func=entropy)\n",
    "\n",
    "# Print the accuracy report and classification report\n",
    "print(accuracy_report)\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of questions = 1000\n",
      "Training...\n",
      "Predicting...\n",
      "Prediction done...\n",
      "{'ABBR': 0.6666666666666666, 'DESC': 0.8188405797101449, 'ENTY': 0.7340425531914894, 'HUM': 0.8, 'LOC': 0.654320987654321, 'NUM': 0.7876106194690266}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        ABBR       0.86      0.67      0.75         9\n",
      "        DESC       0.75      0.82      0.78       138\n",
      "        ENTY       0.50      0.73      0.60        94\n",
      "         HUM       0.96      0.80      0.87        65\n",
      "         LOC       0.88      0.65      0.75        81\n",
      "         NUM       0.98      0.79      0.87       113\n",
      "\n",
      "    accuracy                           0.76       500\n",
      "   macro avg       0.82      0.74      0.77       500\n",
      "weighted avg       0.81      0.76      0.77       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Call getReport without using the length feature and POS feature, and with misclassification error as the impurity measure\n",
    "accuracy_report, class_report, root, prediction, actual = getReport(train_data=data, test_data=test_data, lenFlag=False, posFlag=False, func=misclassifcation_error)\n",
    "\n",
    "# Print the accuracy report and classification report\n",
    "print(accuracy_report)\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of questions = 1501\n",
      "Training...\n",
      "Predicting...\n",
      "Prediction done...\n",
      "Len of wrong data for gini 88\n",
      "No of questions = 1501\n",
      "Training...\n",
      "Predicting...\n",
      "Prediction done...\n",
      "Len of wrong data for entropy is 78\n",
      "No of questions = 1501\n",
      "Training...\n",
      "Predicting...\n",
      "Prediction done...\n",
      "Len of wrong data for misclassifcation_error is 78\n"
     ]
    }
   ],
   "source": [
    "# Function to collect data entries with wrong predictions\n",
    "def get_wrong_prediction(prediction, actual, dataset):\n",
    "    data_list = [dataset[i] for i in range(len(prediction)) if prediction[i] != actual[i]]\n",
    "    return data_list\n",
    "\n",
    "# Get accuracy, class matrix, and decision tree for Gini impurity\n",
    "_ , class_matrix, root_gini, prediction_gini, actual_gini  = getReport(train_data=data, test_data=test_data)\n",
    "wrong_data = get_wrong_prediction(prediction_gini, actual_gini, test_data)\n",
    "\n",
    "# Print the number of wrong predictions for Gini impurity\n",
    "print('Len of wrong data for gini', len(wrong_data))\n",
    "\n",
    "# Get accuracy, class matrix, and decision tree for Entropy impurity using wrong predictions from Gini\n",
    "_ , class_matrix, root_entropy, prediction_entropy, actual_entropy  = getReport(train_data=data, test_data=wrong_data, func=entropy)\n",
    "wrong_data_en = get_wrong_prediction(prediction_entropy, actual_entropy, wrong_data)\n",
    "\n",
    "# Print the number of wrong predictions for Entropy impurity\n",
    "print('Len of wrong data for entropy is', len(wrong_data_en))\n",
    "\n",
    "# Get accuracy, class matrix, and decision tree for Misclassification Error impurity using wrong predictions from Gini\n",
    "_ , class_matrix, root_mis, prediction_mis, actual_mis  = getReport(train_data=data, test_data=wrong_data, func=misclassifcation_error)\n",
    "wrong_data_mis = get_wrong_prediction(prediction_entropy, actual_entropy, wrong_data)\n",
    "\n",
    "# Print the number of wrong predictions for Misclassification Error impurity\n",
    "print('Len of wrong data for misclassifcation_error is', len(wrong_data_mis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy correctly classifies 10 as compared to GINI metric\n",
      "Misclassification error correctly classifies 10 as compared to GINI metric\n"
     ]
    }
   ],
   "source": [
    "# Print the number of instances correctly classified by different impurity measures compared to GINI\n",
    "print('Entropy correctly classifies', (len(wrong_data) - len(wrong_data_en)), 'as compared to GINI metric')\n",
    "print('Misclassification error correctly classifies', (len(wrong_data) - len(wrong_data_mis)), 'as compared to GINI metric')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
